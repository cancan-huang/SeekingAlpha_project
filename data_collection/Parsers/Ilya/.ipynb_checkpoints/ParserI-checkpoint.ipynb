{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from user_agent import generate_user_agent\n",
    "\n",
    "def header_change():\n",
    "    header1 = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/' +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             '.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246'}\n",
    "    header2 = {'User-Agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/' +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             '.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36'}\n",
    "    header3 = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/' +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             '.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9'}\n",
    "    header4 = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/' +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             '.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36'}\n",
    "    header5 = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/' +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             str(np.random.randint(1, high=9)) +\n",
    "                             '.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "    headers = [header1, header2, header3, header4, header5]\n",
    "    while True:\n",
    "        for h in headers:\n",
    "            yield h\n",
    "\n",
    "url_art = 'https://seekingalpha.com/earnings/earnings-call-transcripts/'  # + number\n",
    "url_site = 'https://seekingalpha.com/'\n",
    "\n",
    "folder = '../'\n",
    "\n",
    "# headers = header_change()\n",
    "\n",
    "for i in range(3329, 3500):\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    try:\n",
    "        h = generate_user_agent() #next(headers)\n",
    "        page = requests.get(url_art + str(i), headers={'User-Agent': h})\n",
    "        page_bs = BeautifulSoup(page.text, \"lxml\")\n",
    "        print(page_bs)\n",
    "        links = page_bs.findAll('ul', id='analysis-list-container')[0]\n",
    "        links = links.findAll('h3')\n",
    "        arts = []\n",
    "        for a in links:\n",
    "            arts.append(a.findAll('a')[0]['href'])\n",
    "        for x in range(0, len(arts)):\n",
    "            h = generate_user_agent() #next(headers)\n",
    "            page_art = requests.get(url_site + arts[x], headers={'User-Agent': h})\n",
    "            try:\n",
    "                if page_art.status_code == 200:\n",
    "                    page_bs_art = BeautifulSoup(page_art.text, \"lxml\")\n",
    "                    text = page_bs_art.findAll('article')[0]\n",
    "\n",
    "                    with open(folder + \"data/parsing/\" + str(i) + '_num_' + str(x) + '.txt', \"w\") as f:\n",
    "                        f.write(str(text))\n",
    "\n",
    "                    print(str(i) + '_num_' + str(x) + '.txt')\n",
    "\n",
    "                else:\n",
    "                    h = generate_user_agent() #next(headers)\n",
    "                    time.sleep(15)\n",
    "                    page_art = requests.get(url_site + arts[x], headers={'User-Agent': h})\n",
    "                    page_bs_art = BeautifulSoup(page_art.text, \"lxml\")\n",
    "                    text = page_bs_art.findAll('article')[0]\n",
    "\n",
    "                    with open(folder + \"data/parsing/\" + str(i) + '_num_' + str(x) + '.txt', \"w\") as f:\n",
    "                        f.write(str(text))\n",
    "\n",
    "                    print(str(i) + '_num_' + str(x) + '.txt')\n",
    "\n",
    "            except:\n",
    "                h = generate_user_agent() #next(headers)\n",
    "                time.sleep(50)\n",
    "                with open(folder + \"data/errors.txt\", \"a\") as f:\n",
    "                    f.write(url_site + arts[x])\n",
    "\n",
    "    except:\n",
    "        time.sleep(50)\n",
    "        with open(folder + \"data/big_errors.txt\", \"a\") as f:\n",
    "            f.write(url_site + str(i))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
